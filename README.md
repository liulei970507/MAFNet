# [MAFNet](https://arxiv.org/pdf/2312.14446)
Official PyTorch implementation of "Cross-Modal Object Tracking via Modality-Aware Fusion Network and A Large-Scale Dataset". (TNNLS 2024)

### Citation
If you're using this code in a publication, please cite our paper.

	@inproceedings{liu2024cross,
                  title={Cross-Modal Object Tracking via Modality-Aware Fusion Network and A Large-Scale Dataset},
                  author={Liu, Lei and Zhang, Mengya and Li, Cheng and Li, Chenglong and Tang, Jin},
                  booktitle={IEEE Transactions on Neural Networks and Learning Systems},
                  year={2024}
                }
  
### System Requirements are the same as [DiMP](https://github.com/visionml/pytracking).

**Pretrained Model and results**
If you only run the tracker, you can use the pretrained model: 
[Google Drive](https://drive.google.com/drive/folders/1devKVDIBG7vf7aRa-JbUT0dFlAAOoi_y?usp=drive_link)/[Baidu Yun](https://pan.baidu.com/s/1IlK-zaEVgMmKuFunGPt7Iw?pwd=52zi).
Also, results from pretrained model are provided in [Baidu Yun](https://pan.baidu.com/share/init?surl=7YPR9y3XfQpSONfXL_RHCQ)(Code:xxv0), more details could be found in [hear](https://github.com/mmic-lcl/Datasets-and-benchmark-code).

